{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_channels != out_channels:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_channels)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = self.relu(out)\n",
    "        return out\n",
    "    \n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, inital_channel = 7, num_blocks=20, num_classes=15):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_channels = 256\n",
    "        self.value_num = 256\n",
    "        self.policy_num = 2\n",
    "        self.classes = num_classes**2\n",
    "\n",
    "        #initial block\n",
    "        self.conv1 = nn.Conv2d(inital_channel, self.in_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(self.in_channels)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        #RNN Blocks\n",
    "        self.layers = []\n",
    "        for _ in range(num_blocks):\n",
    "            self.layers.append(block(self.in_channels, self.in_channels))\n",
    "        self.layers = nn.Sequential(*self.layers)\n",
    "\n",
    "        # Policy Block\n",
    "        self.policy_conv = nn.Conv2d(self.in_channels, self.policy_num, kernel_size=1, stride=1, padding=1, bias = False)\n",
    "        self.policy_bn = nn.BatchNorm2d(self.policy_num)\n",
    "        self.policy_relu = nn.ReLU()\n",
    "        self.policy_linear = nn.Linear((self.policy_num+num_classes)**2*self.policy_num, self.classes)  # 수정된 부분\n",
    "\n",
    "        # Value Block\n",
    "        self.value_conv = nn.Conv2d(self.in_channels, 1, kernel_size=1, stride=1, padding = 1, bias=False)\n",
    "        self.value_bn1 = nn.BatchNorm2d(1)\n",
    "        self.value_linear = nn.Linear((self.policy_num+num_classes)**2, 1)  # 수정된 부분\n",
    "        self.value_relu = nn.ReLU()\n",
    "        self.value_output = nn.Tanh()\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Initial block\n",
    "        out = self.relu(self.bn1(self.conv1(x)))\n",
    "\n",
    "        # Residual blocks\n",
    "        out = self.layers(out)\n",
    "\n",
    "        # Policy head\n",
    "        policy = self.policy_relu(self.policy_bn(self.policy_conv(out)))\n",
    "        policy = policy.view(policy.size(0), -1)  # 평탄화\n",
    "        policy = self.policy_linear(policy)  # 최종 정책 출력\n",
    "\n",
    "        # Value head\n",
    "        value = self.value_relu(self.value_bn1(self.value_conv(out)))\n",
    "        value = value.view(value.size(0), -1)  # 평탄화\n",
    "        value = self.value_linear(value)\n",
    "        value = self.value_relu(value)\n",
    "        value = self.value_output(value)  # 최종 값 출력\n",
    "\n",
    "        return policy, value\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(7, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU()\n",
       "  (layers): Sequential(\n",
       "    (0): ResidualBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "    (1): ResidualBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "    (2): ResidualBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "    (3): ResidualBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "    (4): ResidualBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "    (5): ResidualBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "    (6): ResidualBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "    (7): ResidualBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "    (8): ResidualBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "    (9): ResidualBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "    (10): ResidualBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "    (11): ResidualBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "    (12): ResidualBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "    (13): ResidualBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "    (14): ResidualBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "    (15): ResidualBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "    (16): ResidualBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "    (17): ResidualBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "    (18): ResidualBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "    (19): ResidualBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "  )\n",
       "  (policy_conv): Conv2d(256, 2, kernel_size=(1, 1), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (policy_bn): BatchNorm2d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (policy_relu): ReLU()\n",
       "  (policy_linear): Linear(in_features=578, out_features=225, bias=True)\n",
       "  (value_conv): Conv2d(256, 1, kernel_size=(1, 1), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (value_bn1): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (value_linear): Linear(in_features=289, out_features=1, bias=True)\n",
       "  (value_relu): ReLU()\n",
       "  (value_output): Tanh()\n",
       ")"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "model = ResNet(ResidualBlock)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "test_board = np.array([[[0 for _ in range (15)] for _ in range (15)] for _ in range (7)])\n",
    "board_tensor = torch.tensor(test_board, dtype=torch.float32)\n",
    "\n",
    "\n",
    "# 모델, 테스트 데이터를 GPU로 이동\n",
    "board_tensor = board_tensor.to(device)\n",
    "board_tensor = board_tensor.unsqueeze(0) \n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1          [-1, 256, 15, 15]          16,128\n",
      "       BatchNorm2d-2          [-1, 256, 15, 15]             512\n",
      "              ReLU-3          [-1, 256, 15, 15]               0\n",
      "            Conv2d-4          [-1, 256, 15, 15]         589,824\n",
      "       BatchNorm2d-5          [-1, 256, 15, 15]             512\n",
      "              ReLU-6          [-1, 256, 15, 15]               0\n",
      "            Conv2d-7          [-1, 256, 15, 15]         589,824\n",
      "       BatchNorm2d-8          [-1, 256, 15, 15]             512\n",
      "              ReLU-9          [-1, 256, 15, 15]               0\n",
      "    ResidualBlock-10          [-1, 256, 15, 15]               0\n",
      "           Conv2d-11          [-1, 256, 15, 15]         589,824\n",
      "      BatchNorm2d-12          [-1, 256, 15, 15]             512\n",
      "             ReLU-13          [-1, 256, 15, 15]               0\n",
      "           Conv2d-14          [-1, 256, 15, 15]         589,824\n",
      "      BatchNorm2d-15          [-1, 256, 15, 15]             512\n",
      "             ReLU-16          [-1, 256, 15, 15]               0\n",
      "    ResidualBlock-17          [-1, 256, 15, 15]               0\n",
      "           Conv2d-18          [-1, 256, 15, 15]         589,824\n",
      "      BatchNorm2d-19          [-1, 256, 15, 15]             512\n",
      "             ReLU-20          [-1, 256, 15, 15]               0\n",
      "           Conv2d-21          [-1, 256, 15, 15]         589,824\n",
      "      BatchNorm2d-22          [-1, 256, 15, 15]             512\n",
      "             ReLU-23          [-1, 256, 15, 15]               0\n",
      "    ResidualBlock-24          [-1, 256, 15, 15]               0\n",
      "           Conv2d-25          [-1, 256, 15, 15]         589,824\n",
      "      BatchNorm2d-26          [-1, 256, 15, 15]             512\n",
      "             ReLU-27          [-1, 256, 15, 15]               0\n",
      "           Conv2d-28          [-1, 256, 15, 15]         589,824\n",
      "      BatchNorm2d-29          [-1, 256, 15, 15]             512\n",
      "             ReLU-30          [-1, 256, 15, 15]               0\n",
      "    ResidualBlock-31          [-1, 256, 15, 15]               0\n",
      "           Conv2d-32          [-1, 256, 15, 15]         589,824\n",
      "      BatchNorm2d-33          [-1, 256, 15, 15]             512\n",
      "             ReLU-34          [-1, 256, 15, 15]               0\n",
      "           Conv2d-35          [-1, 256, 15, 15]         589,824\n",
      "      BatchNorm2d-36          [-1, 256, 15, 15]             512\n",
      "             ReLU-37          [-1, 256, 15, 15]               0\n",
      "    ResidualBlock-38          [-1, 256, 15, 15]               0\n",
      "           Conv2d-39          [-1, 256, 15, 15]         589,824\n",
      "      BatchNorm2d-40          [-1, 256, 15, 15]             512\n",
      "             ReLU-41          [-1, 256, 15, 15]               0\n",
      "           Conv2d-42          [-1, 256, 15, 15]         589,824\n",
      "      BatchNorm2d-43          [-1, 256, 15, 15]             512\n",
      "             ReLU-44          [-1, 256, 15, 15]               0\n",
      "    ResidualBlock-45          [-1, 256, 15, 15]               0\n",
      "           Conv2d-46          [-1, 256, 15, 15]         589,824\n",
      "      BatchNorm2d-47          [-1, 256, 15, 15]             512\n",
      "             ReLU-48          [-1, 256, 15, 15]               0\n",
      "           Conv2d-49          [-1, 256, 15, 15]         589,824\n",
      "      BatchNorm2d-50          [-1, 256, 15, 15]             512\n",
      "             ReLU-51          [-1, 256, 15, 15]               0\n",
      "    ResidualBlock-52          [-1, 256, 15, 15]               0\n",
      "           Conv2d-53          [-1, 256, 15, 15]         589,824\n",
      "      BatchNorm2d-54          [-1, 256, 15, 15]             512\n",
      "             ReLU-55          [-1, 256, 15, 15]               0\n",
      "           Conv2d-56          [-1, 256, 15, 15]         589,824\n",
      "      BatchNorm2d-57          [-1, 256, 15, 15]             512\n",
      "             ReLU-58          [-1, 256, 15, 15]               0\n",
      "    ResidualBlock-59          [-1, 256, 15, 15]               0\n",
      "           Conv2d-60          [-1, 256, 15, 15]         589,824\n",
      "      BatchNorm2d-61          [-1, 256, 15, 15]             512\n",
      "             ReLU-62          [-1, 256, 15, 15]               0\n",
      "           Conv2d-63          [-1, 256, 15, 15]         589,824\n",
      "      BatchNorm2d-64          [-1, 256, 15, 15]             512\n",
      "             ReLU-65          [-1, 256, 15, 15]               0\n",
      "    ResidualBlock-66          [-1, 256, 15, 15]               0\n",
      "           Conv2d-67          [-1, 256, 15, 15]         589,824\n",
      "      BatchNorm2d-68          [-1, 256, 15, 15]             512\n",
      "             ReLU-69          [-1, 256, 15, 15]               0\n",
      "           Conv2d-70          [-1, 256, 15, 15]         589,824\n",
      "      BatchNorm2d-71          [-1, 256, 15, 15]             512\n",
      "             ReLU-72          [-1, 256, 15, 15]               0\n",
      "    ResidualBlock-73          [-1, 256, 15, 15]               0\n",
      "           Conv2d-74          [-1, 256, 15, 15]         589,824\n",
      "      BatchNorm2d-75          [-1, 256, 15, 15]             512\n",
      "             ReLU-76          [-1, 256, 15, 15]               0\n",
      "           Conv2d-77          [-1, 256, 15, 15]         589,824\n",
      "      BatchNorm2d-78          [-1, 256, 15, 15]             512\n",
      "             ReLU-79          [-1, 256, 15, 15]               0\n",
      "    ResidualBlock-80          [-1, 256, 15, 15]               0\n",
      "           Conv2d-81          [-1, 256, 15, 15]         589,824\n",
      "      BatchNorm2d-82          [-1, 256, 15, 15]             512\n",
      "             ReLU-83          [-1, 256, 15, 15]               0\n",
      "           Conv2d-84          [-1, 256, 15, 15]         589,824\n",
      "      BatchNorm2d-85          [-1, 256, 15, 15]             512\n",
      "             ReLU-86          [-1, 256, 15, 15]               0\n",
      "    ResidualBlock-87          [-1, 256, 15, 15]               0\n",
      "           Conv2d-88          [-1, 256, 15, 15]         589,824\n",
      "      BatchNorm2d-89          [-1, 256, 15, 15]             512\n",
      "             ReLU-90          [-1, 256, 15, 15]               0\n",
      "           Conv2d-91          [-1, 256, 15, 15]         589,824\n",
      "      BatchNorm2d-92          [-1, 256, 15, 15]             512\n",
      "             ReLU-93          [-1, 256, 15, 15]               0\n",
      "    ResidualBlock-94          [-1, 256, 15, 15]               0\n",
      "           Conv2d-95          [-1, 256, 15, 15]         589,824\n",
      "      BatchNorm2d-96          [-1, 256, 15, 15]             512\n",
      "             ReLU-97          [-1, 256, 15, 15]               0\n",
      "           Conv2d-98          [-1, 256, 15, 15]         589,824\n",
      "      BatchNorm2d-99          [-1, 256, 15, 15]             512\n",
      "            ReLU-100          [-1, 256, 15, 15]               0\n",
      "   ResidualBlock-101          [-1, 256, 15, 15]               0\n",
      "          Conv2d-102          [-1, 256, 15, 15]         589,824\n",
      "     BatchNorm2d-103          [-1, 256, 15, 15]             512\n",
      "            ReLU-104          [-1, 256, 15, 15]               0\n",
      "          Conv2d-105          [-1, 256, 15, 15]         589,824\n",
      "     BatchNorm2d-106          [-1, 256, 15, 15]             512\n",
      "            ReLU-107          [-1, 256, 15, 15]               0\n",
      "   ResidualBlock-108          [-1, 256, 15, 15]               0\n",
      "          Conv2d-109          [-1, 256, 15, 15]         589,824\n",
      "     BatchNorm2d-110          [-1, 256, 15, 15]             512\n",
      "            ReLU-111          [-1, 256, 15, 15]               0\n",
      "          Conv2d-112          [-1, 256, 15, 15]         589,824\n",
      "     BatchNorm2d-113          [-1, 256, 15, 15]             512\n",
      "            ReLU-114          [-1, 256, 15, 15]               0\n",
      "   ResidualBlock-115          [-1, 256, 15, 15]               0\n",
      "          Conv2d-116          [-1, 256, 15, 15]         589,824\n",
      "     BatchNorm2d-117          [-1, 256, 15, 15]             512\n",
      "            ReLU-118          [-1, 256, 15, 15]               0\n",
      "          Conv2d-119          [-1, 256, 15, 15]         589,824\n",
      "     BatchNorm2d-120          [-1, 256, 15, 15]             512\n",
      "            ReLU-121          [-1, 256, 15, 15]               0\n",
      "   ResidualBlock-122          [-1, 256, 15, 15]               0\n",
      "          Conv2d-123          [-1, 256, 15, 15]         589,824\n",
      "     BatchNorm2d-124          [-1, 256, 15, 15]             512\n",
      "            ReLU-125          [-1, 256, 15, 15]               0\n",
      "          Conv2d-126          [-1, 256, 15, 15]         589,824\n",
      "     BatchNorm2d-127          [-1, 256, 15, 15]             512\n",
      "            ReLU-128          [-1, 256, 15, 15]               0\n",
      "   ResidualBlock-129          [-1, 256, 15, 15]               0\n",
      "          Conv2d-130          [-1, 256, 15, 15]         589,824\n",
      "     BatchNorm2d-131          [-1, 256, 15, 15]             512\n",
      "            ReLU-132          [-1, 256, 15, 15]               0\n",
      "          Conv2d-133          [-1, 256, 15, 15]         589,824\n",
      "     BatchNorm2d-134          [-1, 256, 15, 15]             512\n",
      "            ReLU-135          [-1, 256, 15, 15]               0\n",
      "   ResidualBlock-136          [-1, 256, 15, 15]               0\n",
      "          Conv2d-137          [-1, 256, 15, 15]         589,824\n",
      "     BatchNorm2d-138          [-1, 256, 15, 15]             512\n",
      "            ReLU-139          [-1, 256, 15, 15]               0\n",
      "          Conv2d-140          [-1, 256, 15, 15]         589,824\n",
      "     BatchNorm2d-141          [-1, 256, 15, 15]             512\n",
      "            ReLU-142          [-1, 256, 15, 15]               0\n",
      "   ResidualBlock-143          [-1, 256, 15, 15]               0\n",
      "          Conv2d-144            [-1, 2, 17, 17]             512\n",
      "     BatchNorm2d-145            [-1, 2, 17, 17]               4\n",
      "            ReLU-146            [-1, 2, 17, 17]               0\n",
      "          Linear-147                  [-1, 225]         130,275\n",
      "          Conv2d-148            [-1, 1, 17, 17]             256\n",
      "     BatchNorm2d-149            [-1, 1, 17, 17]               2\n",
      "            ReLU-150            [-1, 1, 17, 17]               0\n",
      "          Linear-151                    [-1, 1]             290\n",
      "            ReLU-152                    [-1, 1]               0\n",
      "            Tanh-153                    [-1, 1]               0\n",
      "================================================================\n",
      "Total params: 23,761,419\n",
      "Trainable params: 23,761,419\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 62.86\n",
      "Params size (MB): 90.64\n",
      "Estimated Total Size (MB): 153.51\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "summary(model, input_size=(7, 15, 15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "# 모델의 첫 번째 파라미터가 위치한 디바이스 확인\n",
    "model_device = next(model.parameters()).device\n",
    "print(model_device)\n",
    "\n",
    "# board_tensor의 디바이스 확인\n",
    "print(board_tensor.device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():  # Gradient 계산 비활성화\n",
    "    policy, value = model(board_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.7608e-03,  8.5580e-03, -4.0488e-03, -3.4897e-02,  5.1077e-02,\n",
      "          3.1379e-02, -1.6601e-02,  2.1862e-02, -1.9335e-02, -3.0400e-02,\n",
      "          5.4215e-02, -6.3216e-03, -7.9829e-03,  5.2556e-03,  2.8475e-02,\n",
      "          2.8338e-02,  1.9801e-02,  5.4724e-03,  5.1513e-02, -2.1454e-02,\n",
      "         -3.4921e-02, -3.8315e-02,  3.9937e-02, -5.0859e-02,  3.4419e-02,\n",
      "         -3.2661e-02, -2.3469e-02, -3.6093e-02,  2.9734e-02, -1.9743e-02,\n",
      "          4.1360e-02,  1.7285e-02, -1.0399e-02, -2.4012e-02, -1.0489e-02,\n",
      "         -1.3591e-02, -4.9252e-02, -3.0738e-02, -3.7061e-02,  1.3450e-02,\n",
      "         -2.6896e-02,  6.3587e-03, -3.1419e-02, -2.5669e-02, -2.8090e-02,\n",
      "         -2.4290e-02,  1.8134e-02,  2.2544e-02, -5.7966e-02,  3.7540e-02,\n",
      "          4.3163e-02, -2.7966e-02,  8.6479e-03,  3.5397e-02, -3.1726e-02,\n",
      "         -2.2607e-02, -3.1928e-02, -2.8477e-02,  2.6253e-02,  2.6936e-02,\n",
      "          2.8879e-02,  2.1345e-02, -4.0308e-02,  2.1084e-02, -6.0633e-02,\n",
      "          1.9520e-02,  1.9328e-02, -1.0620e-02, -2.1915e-04, -1.5407e-02,\n",
      "          4.8475e-03,  3.8059e-03,  3.2638e-02,  9.1165e-03,  1.2712e-02,\n",
      "          6.8770e-03, -2.3335e-02,  1.1777e-02, -3.6427e-02, -2.6450e-02,\n",
      "         -5.7341e-02,  4.3117e-02, -5.0873e-02, -3.1345e-02,  6.1746e-03,\n",
      "          1.4591e-02,  4.3093e-02, -1.9758e-02,  2.6956e-02, -5.0771e-03,\n",
      "         -2.9558e-02, -2.0857e-02,  7.4958e-03, -2.1409e-02,  2.2505e-02,\n",
      "         -3.0911e-02,  1.8804e-03, -4.6019e-02, -2.3105e-02, -1.7286e-03,\n",
      "          2.3321e-02, -2.7188e-02,  9.0840e-03,  3.6876e-03, -3.1388e-02,\n",
      "          3.1126e-02, -3.1218e-02,  3.1700e-03,  7.1486e-03, -1.6123e-02,\n",
      "          3.9739e-02, -1.2964e-02,  1.0692e-02, -4.7940e-02,  1.3176e-02,\n",
      "          5.0754e-02, -1.9578e-02, -5.6788e-02, -8.6934e-03,  4.8267e-03,\n",
      "          1.8551e-02, -4.6338e-03, -5.8020e-02, -1.7387e-02,  2.1237e-02,\n",
      "          2.6803e-02, -1.1043e-02, -3.6272e-02, -3.4748e-02,  2.7099e-02,\n",
      "         -2.4905e-02, -8.1860e-03, -2.8390e-02, -5.7915e-02, -3.1293e-02,\n",
      "         -8.7681e-03,  1.9323e-02, -2.7195e-02, -3.9551e-02,  2.1256e-02,\n",
      "         -1.6778e-02, -2.8379e-02,  1.8530e-02, -7.6964e-03,  3.6853e-02,\n",
      "         -5.4926e-02, -1.6253e-02, -6.6028e-03,  1.2552e-02, -2.2456e-02,\n",
      "         -1.3424e-02, -3.7609e-02, -4.7324e-02,  1.5413e-02,  2.7931e-02,\n",
      "         -1.0435e-02, -1.0706e-02, -2.7362e-02,  3.8670e-03, -1.9129e-02,\n",
      "          5.5239e-02,  3.5297e-02, -4.9668e-02,  2.5564e-02,  3.1532e-02,\n",
      "         -4.3978e-02,  1.7782e-03, -4.2795e-02, -2.0726e-02,  2.6180e-02,\n",
      "          3.7605e-02,  2.1754e-02, -6.6475e-02,  1.7970e-02, -2.0925e-02,\n",
      "          3.0488e-02,  8.6172e-03, -2.3181e-02,  9.6363e-03, -2.2930e-02,\n",
      "         -2.8581e-02,  1.5442e-02, -2.6228e-03, -7.5333e-03,  2.4708e-02,\n",
      "          2.4865e-02,  3.8588e-02,  2.0839e-02, -3.8950e-02,  7.9054e-03,\n",
      "         -1.0777e-02, -2.0840e-02,  8.2644e-03,  1.5036e-02, -2.4997e-02,\n",
      "         -3.8235e-02,  1.6059e-02,  2.0555e-02,  1.8344e-02,  3.1500e-02,\n",
      "          1.5253e-02, -4.7695e-02,  9.0263e-04, -6.1855e-05,  2.5854e-02,\n",
      "          1.0961e-02,  3.1355e-02,  4.1726e-02, -3.7525e-02, -3.5214e-04,\n",
      "          2.2039e-02, -3.0096e-02,  4.7810e-02, -1.2962e-02,  1.3119e-03,\n",
      "          4.7974e-02, -1.2414e-02, -2.5111e-03,  2.5999e-02, -7.4590e-03,\n",
      "         -1.3397e-02,  1.1927e-02,  2.5739e-02,  5.2674e-02, -3.8445e-03]],\n",
      "       device='cuda:0')\n",
      "225\n"
     ]
    }
   ],
   "source": [
    "print(policy)\n",
    "print(len(policy[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0027]], device='cuda:0')\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "print(value)\n",
    "print(len(value[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
