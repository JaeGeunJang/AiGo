{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_channels != out_channels:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_channels)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = self.relu(out)\n",
    "        return out\n",
    "    \n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, inital_channel = 7, num_blocks=20, num_classes=15):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_channels = 256\n",
    "        self.value_num = 256\n",
    "        self.policy_num = 2\n",
    "        self.classes = num_classes**2\n",
    "\n",
    "        #initial block\n",
    "        self.conv1 = nn.Conv2d(inital_channel, self.in_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(self.in_channels)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        #RNN Blocks\n",
    "        self.layers = []\n",
    "        for _ in range(num_blocks):\n",
    "            self.layers.append(block(self.in_channels, self.in_channels))\n",
    "        self.layers = nn.Sequential(*self.layers)\n",
    "\n",
    "        # Policy Block\n",
    "        self.policy_conv = nn.Conv2d(self.in_channels, self.policy_num, kernel_size=1, stride=1, padding=1, bias = False)\n",
    "        self.policy_bn = nn.BatchNorm2d(self.policy_num)\n",
    "        self.policy_relu = nn.ReLU()\n",
    "        self.policy_linear = nn.Linear((self.policy_num+num_classes)**2*self.policy_num, self.classes)  # 수정된 부분\n",
    "\n",
    "        # Value Block\n",
    "        self.value_conv = nn.Conv2d(self.in_channels, 1, kernel_size=1, stride=1, padding = 1, bias=False)\n",
    "        self.value_bn1 = nn.BatchNorm2d(1)\n",
    "        self.value_linear = nn.Linear((self.policy_num+num_classes)**2, self.value_num)  # 수정된 부분\n",
    "        self.value_relu = nn.ReLU()\n",
    "        self.value_output = nn.Tanh()\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Initial block\n",
    "        out = self.relu(self.bn1(self.conv1(x)))\n",
    "\n",
    "        # Residual blocks\n",
    "        out = self.layers(out)\n",
    "\n",
    "        # Policy head\n",
    "        policy = self.policy_relu(self.policy_bn(self.policy_conv(out)))\n",
    "        policy = policy.view(policy.size(0), -1)  # 평탄화\n",
    "        policy = self.policy_linear(policy)  # 최종 정책 출력\n",
    "\n",
    "        # Value head\n",
    "        value = self.value_relu(self.value_bn1(self.value_conv(out)))\n",
    "        value = value.view(value.size(0), -1)  # 평탄화\n",
    "        value = self.value_linear(value)\n",
    "        value = self.value_relu(value)\n",
    "        value = self.value_output(value)  # 최종 값 출력\n",
    "\n",
    "        return policy, value\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(7, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU()\n",
       "  (layers): Sequential(\n",
       "    (0): ResidualBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "    (1): ResidualBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "    (2): ResidualBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "    (3): ResidualBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "    (4): ResidualBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "    (5): ResidualBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "    (6): ResidualBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "    (7): ResidualBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "    (8): ResidualBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "    (9): ResidualBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "    (10): ResidualBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "    (11): ResidualBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "    (12): ResidualBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "    (13): ResidualBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "    (14): ResidualBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "    (15): ResidualBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "    (16): ResidualBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "    (17): ResidualBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "    (18): ResidualBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "    (19): ResidualBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "  )\n",
       "  (policy_conv): Conv2d(256, 2, kernel_size=(1, 1), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (policy_bn): BatchNorm2d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (policy_relu): ReLU()\n",
       "  (policy_linear): Linear(in_features=578, out_features=225, bias=True)\n",
       "  (value_conv): Conv2d(256, 1, kernel_size=(1, 1), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (value_bn1): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (value_linear): Linear(in_features=289, out_features=256, bias=True)\n",
       "  (value_relu): ReLU()\n",
       "  (value_output): Tanh()\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = ResNet(ResidualBlock)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 모델을 GPU로 이동\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1          [-1, 256, 15, 15]          16,128\n",
      "       BatchNorm2d-2          [-1, 256, 15, 15]             512\n",
      "              ReLU-3          [-1, 256, 15, 15]               0\n",
      "            Conv2d-4          [-1, 256, 15, 15]         589,824\n",
      "       BatchNorm2d-5          [-1, 256, 15, 15]             512\n",
      "              ReLU-6          [-1, 256, 15, 15]               0\n",
      "            Conv2d-7          [-1, 256, 15, 15]         589,824\n",
      "       BatchNorm2d-8          [-1, 256, 15, 15]             512\n",
      "              ReLU-9          [-1, 256, 15, 15]               0\n",
      "    ResidualBlock-10          [-1, 256, 15, 15]               0\n",
      "           Conv2d-11          [-1, 256, 15, 15]         589,824\n",
      "      BatchNorm2d-12          [-1, 256, 15, 15]             512\n",
      "             ReLU-13          [-1, 256, 15, 15]               0\n",
      "           Conv2d-14          [-1, 256, 15, 15]         589,824\n",
      "      BatchNorm2d-15          [-1, 256, 15, 15]             512\n",
      "             ReLU-16          [-1, 256, 15, 15]               0\n",
      "    ResidualBlock-17          [-1, 256, 15, 15]               0\n",
      "           Conv2d-18          [-1, 256, 15, 15]         589,824\n",
      "      BatchNorm2d-19          [-1, 256, 15, 15]             512\n",
      "             ReLU-20          [-1, 256, 15, 15]               0\n",
      "           Conv2d-21          [-1, 256, 15, 15]         589,824\n",
      "      BatchNorm2d-22          [-1, 256, 15, 15]             512\n",
      "             ReLU-23          [-1, 256, 15, 15]               0\n",
      "    ResidualBlock-24          [-1, 256, 15, 15]               0\n",
      "           Conv2d-25          [-1, 256, 15, 15]         589,824\n",
      "      BatchNorm2d-26          [-1, 256, 15, 15]             512\n",
      "             ReLU-27          [-1, 256, 15, 15]               0\n",
      "           Conv2d-28          [-1, 256, 15, 15]         589,824\n",
      "      BatchNorm2d-29          [-1, 256, 15, 15]             512\n",
      "             ReLU-30          [-1, 256, 15, 15]               0\n",
      "    ResidualBlock-31          [-1, 256, 15, 15]               0\n",
      "           Conv2d-32          [-1, 256, 15, 15]         589,824\n",
      "      BatchNorm2d-33          [-1, 256, 15, 15]             512\n",
      "             ReLU-34          [-1, 256, 15, 15]               0\n",
      "           Conv2d-35          [-1, 256, 15, 15]         589,824\n",
      "      BatchNorm2d-36          [-1, 256, 15, 15]             512\n",
      "             ReLU-37          [-1, 256, 15, 15]               0\n",
      "    ResidualBlock-38          [-1, 256, 15, 15]               0\n",
      "           Conv2d-39          [-1, 256, 15, 15]         589,824\n",
      "      BatchNorm2d-40          [-1, 256, 15, 15]             512\n",
      "             ReLU-41          [-1, 256, 15, 15]               0\n",
      "           Conv2d-42          [-1, 256, 15, 15]         589,824\n",
      "      BatchNorm2d-43          [-1, 256, 15, 15]             512\n",
      "             ReLU-44          [-1, 256, 15, 15]               0\n",
      "    ResidualBlock-45          [-1, 256, 15, 15]               0\n",
      "           Conv2d-46          [-1, 256, 15, 15]         589,824\n",
      "      BatchNorm2d-47          [-1, 256, 15, 15]             512\n",
      "             ReLU-48          [-1, 256, 15, 15]               0\n",
      "           Conv2d-49          [-1, 256, 15, 15]         589,824\n",
      "      BatchNorm2d-50          [-1, 256, 15, 15]             512\n",
      "             ReLU-51          [-1, 256, 15, 15]               0\n",
      "    ResidualBlock-52          [-1, 256, 15, 15]               0\n",
      "           Conv2d-53          [-1, 256, 15, 15]         589,824\n",
      "      BatchNorm2d-54          [-1, 256, 15, 15]             512\n",
      "             ReLU-55          [-1, 256, 15, 15]               0\n",
      "           Conv2d-56          [-1, 256, 15, 15]         589,824\n",
      "      BatchNorm2d-57          [-1, 256, 15, 15]             512\n",
      "             ReLU-58          [-1, 256, 15, 15]               0\n",
      "    ResidualBlock-59          [-1, 256, 15, 15]               0\n",
      "           Conv2d-60          [-1, 256, 15, 15]         589,824\n",
      "      BatchNorm2d-61          [-1, 256, 15, 15]             512\n",
      "             ReLU-62          [-1, 256, 15, 15]               0\n",
      "           Conv2d-63          [-1, 256, 15, 15]         589,824\n",
      "      BatchNorm2d-64          [-1, 256, 15, 15]             512\n",
      "             ReLU-65          [-1, 256, 15, 15]               0\n",
      "    ResidualBlock-66          [-1, 256, 15, 15]               0\n",
      "           Conv2d-67          [-1, 256, 15, 15]         589,824\n",
      "      BatchNorm2d-68          [-1, 256, 15, 15]             512\n",
      "             ReLU-69          [-1, 256, 15, 15]               0\n",
      "           Conv2d-70          [-1, 256, 15, 15]         589,824\n",
      "      BatchNorm2d-71          [-1, 256, 15, 15]             512\n",
      "             ReLU-72          [-1, 256, 15, 15]               0\n",
      "    ResidualBlock-73          [-1, 256, 15, 15]               0\n",
      "           Conv2d-74          [-1, 256, 15, 15]         589,824\n",
      "      BatchNorm2d-75          [-1, 256, 15, 15]             512\n",
      "             ReLU-76          [-1, 256, 15, 15]               0\n",
      "           Conv2d-77          [-1, 256, 15, 15]         589,824\n",
      "      BatchNorm2d-78          [-1, 256, 15, 15]             512\n",
      "             ReLU-79          [-1, 256, 15, 15]               0\n",
      "    ResidualBlock-80          [-1, 256, 15, 15]               0\n",
      "           Conv2d-81          [-1, 256, 15, 15]         589,824\n",
      "      BatchNorm2d-82          [-1, 256, 15, 15]             512\n",
      "             ReLU-83          [-1, 256, 15, 15]               0\n",
      "           Conv2d-84          [-1, 256, 15, 15]         589,824\n",
      "      BatchNorm2d-85          [-1, 256, 15, 15]             512\n",
      "             ReLU-86          [-1, 256, 15, 15]               0\n",
      "    ResidualBlock-87          [-1, 256, 15, 15]               0\n",
      "           Conv2d-88          [-1, 256, 15, 15]         589,824\n",
      "      BatchNorm2d-89          [-1, 256, 15, 15]             512\n",
      "             ReLU-90          [-1, 256, 15, 15]               0\n",
      "           Conv2d-91          [-1, 256, 15, 15]         589,824\n",
      "      BatchNorm2d-92          [-1, 256, 15, 15]             512\n",
      "             ReLU-93          [-1, 256, 15, 15]               0\n",
      "    ResidualBlock-94          [-1, 256, 15, 15]               0\n",
      "           Conv2d-95          [-1, 256, 15, 15]         589,824\n",
      "      BatchNorm2d-96          [-1, 256, 15, 15]             512\n",
      "             ReLU-97          [-1, 256, 15, 15]               0\n",
      "           Conv2d-98          [-1, 256, 15, 15]         589,824\n",
      "      BatchNorm2d-99          [-1, 256, 15, 15]             512\n",
      "            ReLU-100          [-1, 256, 15, 15]               0\n",
      "   ResidualBlock-101          [-1, 256, 15, 15]               0\n",
      "          Conv2d-102          [-1, 256, 15, 15]         589,824\n",
      "     BatchNorm2d-103          [-1, 256, 15, 15]             512\n",
      "            ReLU-104          [-1, 256, 15, 15]               0\n",
      "          Conv2d-105          [-1, 256, 15, 15]         589,824\n",
      "     BatchNorm2d-106          [-1, 256, 15, 15]             512\n",
      "            ReLU-107          [-1, 256, 15, 15]               0\n",
      "   ResidualBlock-108          [-1, 256, 15, 15]               0\n",
      "          Conv2d-109          [-1, 256, 15, 15]         589,824\n",
      "     BatchNorm2d-110          [-1, 256, 15, 15]             512\n",
      "            ReLU-111          [-1, 256, 15, 15]               0\n",
      "          Conv2d-112          [-1, 256, 15, 15]         589,824\n",
      "     BatchNorm2d-113          [-1, 256, 15, 15]             512\n",
      "            ReLU-114          [-1, 256, 15, 15]               0\n",
      "   ResidualBlock-115          [-1, 256, 15, 15]               0\n",
      "          Conv2d-116          [-1, 256, 15, 15]         589,824\n",
      "     BatchNorm2d-117          [-1, 256, 15, 15]             512\n",
      "            ReLU-118          [-1, 256, 15, 15]               0\n",
      "          Conv2d-119          [-1, 256, 15, 15]         589,824\n",
      "     BatchNorm2d-120          [-1, 256, 15, 15]             512\n",
      "            ReLU-121          [-1, 256, 15, 15]               0\n",
      "   ResidualBlock-122          [-1, 256, 15, 15]               0\n",
      "          Conv2d-123          [-1, 256, 15, 15]         589,824\n",
      "     BatchNorm2d-124          [-1, 256, 15, 15]             512\n",
      "            ReLU-125          [-1, 256, 15, 15]               0\n",
      "          Conv2d-126          [-1, 256, 15, 15]         589,824\n",
      "     BatchNorm2d-127          [-1, 256, 15, 15]             512\n",
      "            ReLU-128          [-1, 256, 15, 15]               0\n",
      "   ResidualBlock-129          [-1, 256, 15, 15]               0\n",
      "          Conv2d-130          [-1, 256, 15, 15]         589,824\n",
      "     BatchNorm2d-131          [-1, 256, 15, 15]             512\n",
      "            ReLU-132          [-1, 256, 15, 15]               0\n",
      "          Conv2d-133          [-1, 256, 15, 15]         589,824\n",
      "     BatchNorm2d-134          [-1, 256, 15, 15]             512\n",
      "            ReLU-135          [-1, 256, 15, 15]               0\n",
      "   ResidualBlock-136          [-1, 256, 15, 15]               0\n",
      "          Conv2d-137          [-1, 256, 15, 15]         589,824\n",
      "     BatchNorm2d-138          [-1, 256, 15, 15]             512\n",
      "            ReLU-139          [-1, 256, 15, 15]               0\n",
      "          Conv2d-140          [-1, 256, 15, 15]         589,824\n",
      "     BatchNorm2d-141          [-1, 256, 15, 15]             512\n",
      "            ReLU-142          [-1, 256, 15, 15]               0\n",
      "   ResidualBlock-143          [-1, 256, 15, 15]               0\n",
      "          Conv2d-144            [-1, 2, 17, 17]             512\n",
      "     BatchNorm2d-145            [-1, 2, 17, 17]               4\n",
      "            ReLU-146            [-1, 2, 17, 17]               0\n",
      "          Linear-147                  [-1, 225]         130,275\n",
      "          Conv2d-148            [-1, 1, 17, 17]             256\n",
      "     BatchNorm2d-149            [-1, 1, 17, 17]               2\n",
      "            ReLU-150            [-1, 1, 17, 17]               0\n",
      "          Linear-151                  [-1, 256]          74,240\n",
      "            ReLU-152                  [-1, 256]               0\n",
      "            Tanh-153                  [-1, 256]               0\n",
      "================================================================\n",
      "Total params: 23,835,369\n",
      "Trainable params: 23,835,369\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 62.87\n",
      "Params size (MB): 90.92\n",
      "Estimated Total Size (MB): 153.80\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "summary(model, input_size=(7, 15, 15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
