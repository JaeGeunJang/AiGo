{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_channels != out_channels:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_channels)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = self.relu(out)\n",
    "        return out\n",
    "    \n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, inital_channel = 7, num_blocks=20, num_classes=15):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_channels = 256\n",
    "        self.value_num = 256\n",
    "        self.policy_num = 2\n",
    "        self.classes = num_classes**2\n",
    "\n",
    "        #initial block\n",
    "        self.conv1 = nn.Conv2d(inital_channel, self.in_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(self.in_channels)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        #RNN Blocks\n",
    "        self.layers = []\n",
    "        for _ in range(num_blocks):\n",
    "            self.layers.append(block(self.in_channels, self.in_channels))\n",
    "        self.layers = nn.Sequential(*self.layers)\n",
    "\n",
    "        # Policy Block\n",
    "        self.policy_conv = nn.Conv2d(self.in_channels, self.policy_num, kernel_size=1, stride=1, padding=1, bias = False)\n",
    "        self.policy_bn = nn.BatchNorm2d(self.policy_num)\n",
    "        self.policy_relu = nn.ReLU()\n",
    "        self.policy_linear = nn.Linear((num_classes+2)**2*self.policy_num, self.classes)  # 수정된 부분\n",
    "\n",
    "        # Value Block\n",
    "        self.value_conv = nn.Conv2d(self.in_channels, 1, kernel_size=1, stride=1, padding = 1, bias=False)\n",
    "        self.value_bn1 = nn.BatchNorm2d(1)\n",
    "        self.value_linear = nn.Linear((num_classes+2)**2, 1)  # 수정된 부분\n",
    "        self.value_relu = nn.ReLU()\n",
    "        self.value_output = nn.Tanh()\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Initial block\n",
    "        out = self.relu(self.bn1(self.conv1(x)))\n",
    "\n",
    "        # Residual blocks\n",
    "        out = self.layers(out)\n",
    "\n",
    "        # Policy head\n",
    "        policy = self.policy_relu(self.policy_bn(self.policy_conv(out)))\n",
    "        policy = policy.view(policy.size(0), -1)  # 평탄화\n",
    "        policy = self.policy_linear(policy)  # 최종 정책 출력\n",
    "\n",
    "        # Value head\n",
    "        value = self.value_relu(self.value_bn1(self.value_conv(out)))\n",
    "        value = value.view(value.size(0), -1)  # 평탄화\n",
    "        value = self.value_linear(value)\n",
    "        value = self.value_relu(value)\n",
    "        value = self.value_output(value)  # 최종 값 출력\n",
    "\n",
    "        return policy, value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(7, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU()\n",
       "  (layers): Sequential(\n",
       "    (0): ResidualBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "    (1): ResidualBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "    (2): ResidualBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "    (3): ResidualBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "    (4): ResidualBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "    (5): ResidualBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "    (6): ResidualBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "    (7): ResidualBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "    (8): ResidualBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "    (9): ResidualBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "    (10): ResidualBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "    (11): ResidualBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "    (12): ResidualBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "    (13): ResidualBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "    (14): ResidualBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "    (15): ResidualBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "    (16): ResidualBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "    (17): ResidualBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "    (18): ResidualBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "    (19): ResidualBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "  )\n",
       "  (policy_conv): Conv2d(256, 2, kernel_size=(1, 1), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (policy_bn): BatchNorm2d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (policy_relu): ReLU()\n",
       "  (policy_linear): Linear(in_features=578, out_features=225, bias=True)\n",
       "  (value_conv): Conv2d(256, 1, kernel_size=(1, 1), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (value_bn1): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (value_linear): Linear(in_features=289, out_features=1, bias=True)\n",
       "  (value_relu): ReLU()\n",
       "  (value_output): Tanh()\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "model = ResNet(ResidualBlock)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "test_board = np.array([[[0 for _ in range (15)] for _ in range (15)] for _ in range (7)])\n",
    "board_tensor = torch.tensor(test_board, dtype=torch.float32)\n",
    "\n",
    "\n",
    "# 모델, 테스트 데이터를 GPU로 이동\n",
    "board_tensor = board_tensor.to(device)\n",
    "board_tensor = board_tensor.unsqueeze(0) \n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1          [-1, 256, 15, 15]          16,128\n",
      "       BatchNorm2d-2          [-1, 256, 15, 15]             512\n",
      "              ReLU-3          [-1, 256, 15, 15]               0\n",
      "            Conv2d-4          [-1, 256, 15, 15]         589,824\n",
      "       BatchNorm2d-5          [-1, 256, 15, 15]             512\n",
      "              ReLU-6          [-1, 256, 15, 15]               0\n",
      "            Conv2d-7          [-1, 256, 15, 15]         589,824\n",
      "       BatchNorm2d-8          [-1, 256, 15, 15]             512\n",
      "              ReLU-9          [-1, 256, 15, 15]               0\n",
      "    ResidualBlock-10          [-1, 256, 15, 15]               0\n",
      "           Conv2d-11          [-1, 256, 15, 15]         589,824\n",
      "      BatchNorm2d-12          [-1, 256, 15, 15]             512\n",
      "             ReLU-13          [-1, 256, 15, 15]               0\n",
      "           Conv2d-14          [-1, 256, 15, 15]         589,824\n",
      "      BatchNorm2d-15          [-1, 256, 15, 15]             512\n",
      "             ReLU-16          [-1, 256, 15, 15]               0\n",
      "    ResidualBlock-17          [-1, 256, 15, 15]               0\n",
      "           Conv2d-18          [-1, 256, 15, 15]         589,824\n",
      "      BatchNorm2d-19          [-1, 256, 15, 15]             512\n",
      "             ReLU-20          [-1, 256, 15, 15]               0\n",
      "           Conv2d-21          [-1, 256, 15, 15]         589,824\n",
      "      BatchNorm2d-22          [-1, 256, 15, 15]             512\n",
      "             ReLU-23          [-1, 256, 15, 15]               0\n",
      "    ResidualBlock-24          [-1, 256, 15, 15]               0\n",
      "           Conv2d-25          [-1, 256, 15, 15]         589,824\n",
      "      BatchNorm2d-26          [-1, 256, 15, 15]             512\n",
      "             ReLU-27          [-1, 256, 15, 15]               0\n",
      "           Conv2d-28          [-1, 256, 15, 15]         589,824\n",
      "      BatchNorm2d-29          [-1, 256, 15, 15]             512\n",
      "             ReLU-30          [-1, 256, 15, 15]               0\n",
      "    ResidualBlock-31          [-1, 256, 15, 15]               0\n",
      "           Conv2d-32          [-1, 256, 15, 15]         589,824\n",
      "      BatchNorm2d-33          [-1, 256, 15, 15]             512\n",
      "             ReLU-34          [-1, 256, 15, 15]               0\n",
      "           Conv2d-35          [-1, 256, 15, 15]         589,824\n",
      "      BatchNorm2d-36          [-1, 256, 15, 15]             512\n",
      "             ReLU-37          [-1, 256, 15, 15]               0\n",
      "    ResidualBlock-38          [-1, 256, 15, 15]               0\n",
      "           Conv2d-39          [-1, 256, 15, 15]         589,824\n",
      "      BatchNorm2d-40          [-1, 256, 15, 15]             512\n",
      "             ReLU-41          [-1, 256, 15, 15]               0\n",
      "           Conv2d-42          [-1, 256, 15, 15]         589,824\n",
      "      BatchNorm2d-43          [-1, 256, 15, 15]             512\n",
      "             ReLU-44          [-1, 256, 15, 15]               0\n",
      "    ResidualBlock-45          [-1, 256, 15, 15]               0\n",
      "           Conv2d-46          [-1, 256, 15, 15]         589,824\n",
      "      BatchNorm2d-47          [-1, 256, 15, 15]             512\n",
      "             ReLU-48          [-1, 256, 15, 15]               0\n",
      "           Conv2d-49          [-1, 256, 15, 15]         589,824\n",
      "      BatchNorm2d-50          [-1, 256, 15, 15]             512\n",
      "             ReLU-51          [-1, 256, 15, 15]               0\n",
      "    ResidualBlock-52          [-1, 256, 15, 15]               0\n",
      "           Conv2d-53          [-1, 256, 15, 15]         589,824\n",
      "      BatchNorm2d-54          [-1, 256, 15, 15]             512\n",
      "             ReLU-55          [-1, 256, 15, 15]               0\n",
      "           Conv2d-56          [-1, 256, 15, 15]         589,824\n",
      "      BatchNorm2d-57          [-1, 256, 15, 15]             512\n",
      "             ReLU-58          [-1, 256, 15, 15]               0\n",
      "    ResidualBlock-59          [-1, 256, 15, 15]               0\n",
      "           Conv2d-60          [-1, 256, 15, 15]         589,824\n",
      "      BatchNorm2d-61          [-1, 256, 15, 15]             512\n",
      "             ReLU-62          [-1, 256, 15, 15]               0\n",
      "           Conv2d-63          [-1, 256, 15, 15]         589,824\n",
      "      BatchNorm2d-64          [-1, 256, 15, 15]             512\n",
      "             ReLU-65          [-1, 256, 15, 15]               0\n",
      "    ResidualBlock-66          [-1, 256, 15, 15]               0\n",
      "           Conv2d-67          [-1, 256, 15, 15]         589,824\n",
      "      BatchNorm2d-68          [-1, 256, 15, 15]             512\n",
      "             ReLU-69          [-1, 256, 15, 15]               0\n",
      "           Conv2d-70          [-1, 256, 15, 15]         589,824\n",
      "      BatchNorm2d-71          [-1, 256, 15, 15]             512\n",
      "             ReLU-72          [-1, 256, 15, 15]               0\n",
      "    ResidualBlock-73          [-1, 256, 15, 15]               0\n",
      "           Conv2d-74          [-1, 256, 15, 15]         589,824\n",
      "      BatchNorm2d-75          [-1, 256, 15, 15]             512\n",
      "             ReLU-76          [-1, 256, 15, 15]               0\n",
      "           Conv2d-77          [-1, 256, 15, 15]         589,824\n",
      "      BatchNorm2d-78          [-1, 256, 15, 15]             512\n",
      "             ReLU-79          [-1, 256, 15, 15]               0\n",
      "    ResidualBlock-80          [-1, 256, 15, 15]               0\n",
      "           Conv2d-81          [-1, 256, 15, 15]         589,824\n",
      "      BatchNorm2d-82          [-1, 256, 15, 15]             512\n",
      "             ReLU-83          [-1, 256, 15, 15]               0\n",
      "           Conv2d-84          [-1, 256, 15, 15]         589,824\n",
      "      BatchNorm2d-85          [-1, 256, 15, 15]             512\n",
      "             ReLU-86          [-1, 256, 15, 15]               0\n",
      "    ResidualBlock-87          [-1, 256, 15, 15]               0\n",
      "           Conv2d-88          [-1, 256, 15, 15]         589,824\n",
      "      BatchNorm2d-89          [-1, 256, 15, 15]             512\n",
      "             ReLU-90          [-1, 256, 15, 15]               0\n",
      "           Conv2d-91          [-1, 256, 15, 15]         589,824\n",
      "      BatchNorm2d-92          [-1, 256, 15, 15]             512\n",
      "             ReLU-93          [-1, 256, 15, 15]               0\n",
      "    ResidualBlock-94          [-1, 256, 15, 15]               0\n",
      "           Conv2d-95          [-1, 256, 15, 15]         589,824\n",
      "      BatchNorm2d-96          [-1, 256, 15, 15]             512\n",
      "             ReLU-97          [-1, 256, 15, 15]               0\n",
      "           Conv2d-98          [-1, 256, 15, 15]         589,824\n",
      "      BatchNorm2d-99          [-1, 256, 15, 15]             512\n",
      "            ReLU-100          [-1, 256, 15, 15]               0\n",
      "   ResidualBlock-101          [-1, 256, 15, 15]               0\n",
      "          Conv2d-102          [-1, 256, 15, 15]         589,824\n",
      "     BatchNorm2d-103          [-1, 256, 15, 15]             512\n",
      "            ReLU-104          [-1, 256, 15, 15]               0\n",
      "          Conv2d-105          [-1, 256, 15, 15]         589,824\n",
      "     BatchNorm2d-106          [-1, 256, 15, 15]             512\n",
      "            ReLU-107          [-1, 256, 15, 15]               0\n",
      "   ResidualBlock-108          [-1, 256, 15, 15]               0\n",
      "          Conv2d-109          [-1, 256, 15, 15]         589,824\n",
      "     BatchNorm2d-110          [-1, 256, 15, 15]             512\n",
      "            ReLU-111          [-1, 256, 15, 15]               0\n",
      "          Conv2d-112          [-1, 256, 15, 15]         589,824\n",
      "     BatchNorm2d-113          [-1, 256, 15, 15]             512\n",
      "            ReLU-114          [-1, 256, 15, 15]               0\n",
      "   ResidualBlock-115          [-1, 256, 15, 15]               0\n",
      "          Conv2d-116          [-1, 256, 15, 15]         589,824\n",
      "     BatchNorm2d-117          [-1, 256, 15, 15]             512\n",
      "            ReLU-118          [-1, 256, 15, 15]               0\n",
      "          Conv2d-119          [-1, 256, 15, 15]         589,824\n",
      "     BatchNorm2d-120          [-1, 256, 15, 15]             512\n",
      "            ReLU-121          [-1, 256, 15, 15]               0\n",
      "   ResidualBlock-122          [-1, 256, 15, 15]               0\n",
      "          Conv2d-123          [-1, 256, 15, 15]         589,824\n",
      "     BatchNorm2d-124          [-1, 256, 15, 15]             512\n",
      "            ReLU-125          [-1, 256, 15, 15]               0\n",
      "          Conv2d-126          [-1, 256, 15, 15]         589,824\n",
      "     BatchNorm2d-127          [-1, 256, 15, 15]             512\n",
      "            ReLU-128          [-1, 256, 15, 15]               0\n",
      "   ResidualBlock-129          [-1, 256, 15, 15]               0\n",
      "          Conv2d-130          [-1, 256, 15, 15]         589,824\n",
      "     BatchNorm2d-131          [-1, 256, 15, 15]             512\n",
      "            ReLU-132          [-1, 256, 15, 15]               0\n",
      "          Conv2d-133          [-1, 256, 15, 15]         589,824\n",
      "     BatchNorm2d-134          [-1, 256, 15, 15]             512\n",
      "            ReLU-135          [-1, 256, 15, 15]               0\n",
      "   ResidualBlock-136          [-1, 256, 15, 15]               0\n",
      "          Conv2d-137          [-1, 256, 15, 15]         589,824\n",
      "     BatchNorm2d-138          [-1, 256, 15, 15]             512\n",
      "            ReLU-139          [-1, 256, 15, 15]               0\n",
      "          Conv2d-140          [-1, 256, 15, 15]         589,824\n",
      "     BatchNorm2d-141          [-1, 256, 15, 15]             512\n",
      "            ReLU-142          [-1, 256, 15, 15]               0\n",
      "   ResidualBlock-143          [-1, 256, 15, 15]               0\n",
      "          Conv2d-144            [-1, 2, 17, 17]             512\n",
      "     BatchNorm2d-145            [-1, 2, 17, 17]               4\n",
      "            ReLU-146            [-1, 2, 17, 17]               0\n",
      "          Linear-147                  [-1, 225]         130,275\n",
      "          Conv2d-148            [-1, 1, 17, 17]             256\n",
      "     BatchNorm2d-149            [-1, 1, 17, 17]               2\n",
      "            ReLU-150            [-1, 1, 17, 17]               0\n",
      "          Linear-151                    [-1, 1]             290\n",
      "            ReLU-152                    [-1, 1]               0\n",
      "            Tanh-153                    [-1, 1]               0\n",
      "================================================================\n",
      "Total params: 23,761,419\n",
      "Trainable params: 23,761,419\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 62.86\n",
      "Params size (MB): 90.64\n",
      "Estimated Total Size (MB): 153.51\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "summary(model, input_size=(7, 15, 15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "# 모델의 첫 번째 파라미터가 위치한 디바이스 확인\n",
    "model_device = next(model.parameters()).device\n",
    "print(model_device)\n",
    "\n",
    "# board_tensor의 디바이스 확인\n",
    "print(board_tensor.device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():  # Gradient 계산 비활성화\n",
    "    policy, value = model(board_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0218, -0.0533,  0.0157,  0.0021,  0.0575, -0.0318,  0.0292, -0.0822,\n",
      "          0.0153, -0.0642, -0.0719,  0.0244,  0.0481, -0.0174,  0.0006, -0.0448,\n",
      "         -0.0305,  0.0359, -0.1112, -0.0356,  0.0637,  0.0625, -0.0367, -0.0009,\n",
      "          0.1019,  0.0459,  0.0156, -0.0762, -0.0582,  0.0782, -0.1073,  0.0592,\n",
      "          0.0390,  0.0531, -0.0113, -0.0924, -0.0076, -0.0262,  0.0036,  0.0226,\n",
      "         -0.0682, -0.0384, -0.0095,  0.0149, -0.0104, -0.0284, -0.0607, -0.0672,\n",
      "          0.0714,  0.0320, -0.0318,  0.0058, -0.0715, -0.1108, -0.0430,  0.0756,\n",
      "         -0.0029,  0.1285,  0.0453, -0.0646, -0.0042,  0.0171,  0.0779,  0.0306,\n",
      "          0.1207,  0.0206,  0.0726,  0.0138,  0.0275,  0.0046, -0.0503,  0.0034,\n",
      "         -0.0197,  0.0232, -0.0279,  0.0500, -0.0624,  0.0471, -0.0226, -0.0258,\n",
      "         -0.0821,  0.0139,  0.0385, -0.0222,  0.0596,  0.0034, -0.1289,  0.0930,\n",
      "         -0.0383,  0.0373, -0.1704, -0.1518,  0.0258,  0.0413, -0.0725, -0.0049,\n",
      "          0.0502, -0.0010,  0.0781, -0.0470, -0.0094,  0.0267, -0.0442, -0.0315,\n",
      "         -0.0408,  0.0105,  0.0163, -0.0210,  0.0263, -0.0760, -0.0162, -0.0376,\n",
      "          0.0752,  0.0155,  0.0305, -0.0295, -0.0329,  0.0987,  0.0053, -0.0241,\n",
      "         -0.0493,  0.0364, -0.0263, -0.0389, -0.0103, -0.0573,  0.0100, -0.0446,\n",
      "         -0.0061, -0.0220, -0.0307,  0.0652,  0.0008,  0.0340,  0.0676,  0.0318,\n",
      "          0.0059,  0.0527, -0.0341,  0.0164, -0.0166,  0.0019, -0.0244,  0.0994,\n",
      "         -0.0037, -0.0008,  0.0336,  0.0036,  0.0211, -0.0169,  0.0689,  0.0281,\n",
      "         -0.0021, -0.1055,  0.0752, -0.0042, -0.1274, -0.0298,  0.1183,  0.0073,\n",
      "         -0.0887,  0.0118,  0.0632,  0.0668, -0.0426,  0.0044,  0.0772, -0.0954,\n",
      "         -0.0977,  0.0035,  0.0703,  0.0086,  0.0182, -0.0486, -0.0483,  0.0150,\n",
      "         -0.0539,  0.0773,  0.1114, -0.0333,  0.0561,  0.0152, -0.1077, -0.0330,\n",
      "         -0.0510, -0.0738,  0.0515, -0.0673, -0.0848, -0.0020, -0.0255, -0.0030,\n",
      "          0.0999,  0.0860, -0.0461, -0.0081,  0.0655, -0.0309, -0.0082,  0.0314,\n",
      "          0.0869,  0.0274,  0.0395, -0.0044,  0.0599, -0.0324,  0.0815, -0.0894,\n",
      "          0.0742, -0.0051, -0.0019, -0.0549,  0.0932, -0.0818,  0.0422,  0.0727,\n",
      "          0.0154,  0.0535,  0.0710, -0.0747, -0.0439, -0.0931,  0.0229, -0.0489,\n",
      "         -0.0377]], device='cuda:0')\n",
      "225\n"
     ]
    }
   ],
   "source": [
    "print(policy)\n",
    "print(len(policy[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0219]], device='cuda:0')\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "print(value)\n",
    "print(len(value[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
